%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  File name: ch2.engl.tex
%  Title:
%  Version: 11.09.2015 (hve)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter[Matrices]{Matrices}
\lb{ch2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this chapter, we introduce a second class of mathematical 
objects that are more general than vectors. For these objects, we 
will also define certain mathematical operations, and a set of 
computational rules that apply in this context.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section[Matrices as linear mappings]%
{Matrices as linear mappings}
\lb{sec:matlinab}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Consider given a collection of $m \times n$ arbitrary real numbers
$a_{11}$, $a_{12}$ \ldots, $a_{ij}$, \ldots,
$a_{mn}$, which we arrange systematically in a particular kind of 
array.

\medskip
\noindent
\underline{\bf Def.:} A real-valued $\boldsymbol{(m\times n)}${\bf -matrix} is formally defined to constitute an array of real 
numbers according to
%
\be
\mathbf{A}
:= \left(\begin{array}{cccccc}
   	a_{11} & a_{12} & \ldots & a_{1j} & \ldots & a_{1n} \\
   	a_{21} & a_{22} & \ldots & a_{2j} & \ldots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
    a_{i1} & a_{i2} & \ldots & a_{ij} & \ldots & a_{in} \\
    \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
    a_{m1} & a_{m2} & \ldots & a_{mj} & \ldots & a_{mn}
	\end{array}\right) \ ,
\ee
%
where $a_{ij} \in {\mathbb R}$,	$i=1, \dots, m; j=1, \dots, n$.

\noindent
Notation: $\mathbf{A} \in \mathbb{R}^{m \times n}$.

\medskip
\noindent
Characteristic features of this array of real numbers are:
%
\begin{itemize}
	\item $m$ denotes the number of {\bf rows} of $\mathbf{A}$, $n$ 
	the number of {\bf columns} of $\mathbf{A}$.
	
	\item $a_{ij}$ represents the {\bf elements} of $\mathbf{A}$;
	$a_{ij}$ is located at the point of intersection of the $i$th 
	row and the $j$th column of $\mathbf{A}$.
	
	\item elements of the $i$th row constitute the {\bf row vector}
	$\left(a_{i1}, a_{i2}, \ldots, a_{ij}, \ldots, a_{in}\right)$,
	elements of the $j$th column the {\bf column vector}
	$\left(\begin{array}{c} a_{1j} \\ a_{2j} \\ \vdots \\ a_{ij} \\
	\vdots \\ a_{mj}
	\end{array}\right)$.
\end{itemize}
%
Formally column vectors need to be viewed as $(n\times 
1)$-matrices, row vectors as $(1\times n)$-matrices. An 
$\boldsymbol{(m \times n)}${\bf -zero matrix}, denoted by
$\mathbf{0}$, has all its elements equal to zero, i.e.,
%
\be
\mathbf{0}
:= \left(\begin{array}{cccc}
   	0 & 0 & \ldots & 0 \\
   	0 & 0 & \ldots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \ldots & 0
	\end{array}\right) \ .
\ee
%

\medskip
\noindent
Matrices which have an \emph{equal} number of rows and columns, 
i.e. $m=n$, are referred to as {\bf quadratic matrices}. In 
particular, the $\boldsymbol{(n \times n)}${\bf -unit matrix} (or 
identity matrix)
%
\be
\lb{einmatr}
\mathbf{1} := \left(\begin{array}{cccccc}
   	1 & 0 & \ldots & 0 & \ldots & 0 \\
   	0 & 1 & \ldots & 0 & \ldots & 0 \\
    \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
    0 & 0 & \ldots & 1 & \ldots & 0 \\
    \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
    0 & 0 & \ldots & 0 & \ldots & 1
	\end{array}\right)
\ee
%
holds a special status in the family of $(n\times n)$-matrices.

\medskip
\noindent
Now we make explicit in what sense we will comprehend $(m\times 
n)$-matrices as mathematical objects.

\medskip
\noindent
\underline{\bf Def.:} A real-valued matrix $\mathbf{A} \in
\mathbb{R}^{m \times n}$ defines by the computational operation
%
\bea
\lb{matabb}
\mathbf{A}\vec{x}
& := & \left(\begin{array}{cccccc}
   	a_{11} & a_{12} & \ldots & a_{1j} & \ldots & a_{1n} \\
   	a_{21} & a_{22} & \ldots & a_{2j} & \ldots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
    a_{i1} & a_{i2} & \ldots & a_{ij} & \ldots & a_{in} \\
    \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
    a_{m1} & a_{m2} & \ldots & a_{mj} & \ldots & a_{mn}
	\end{array}\right)
	\left(\begin{array}{c}
x_{1} \\ x_{2} \\ \vdots \\ x_{j} \\ \vdots \\ x_{n}
\end{array}\right) \nonumber \\
& := & \left(\begin{array}{c}
a_{11}x_{1}+a_{12}x_{2}+\ldots+a_{1j}x_{j}+\ldots+a_{1n}x_{n} \\
a_{21}x_{1}+a_{22}x_{2}+\ldots+a_{2j}x_{j}+\ldots+a_{2n}x_{n} \\
\vdots \\
a_{i1}x_{1}+a_{i2}x_{2}+\ldots+a_{ij}x_{j}+\ldots+a_{in}x_{n} \\
\vdots \\
a_{m1}x_{1}+a_{m2}x_{2}+\ldots+a_{mj}x_{j}+\ldots+a_{mn}x_{n}
\end{array}\right)
%= \left(
%\begin{array}{c}
%\sum_{j=1}^{n}a_{1j}x_{j} \\
%\sum_{j=1}^{n}a_{2j}x_{j} \\
%\vdots \\
%\sum_{j=1}^{n}a_{mj}x_{j}
%\end{array}
%\right)
=: \left(
\begin{array}{c}
y_{1} \\
y_{2} \\
\vdots \\
y_{i} \\
\vdots \\
y_{m}
\end{array}
\right)
= \vec{y}
\eea
%
a {\bf mapping} $\mathbf{A}: \mathbb{R}^{n \times 1} \rightarrow 
\mathbb{R}^{m \times 1}$, i.e. a mapping from the set of 
real-valued $n$-component column vectors (here: $\vec{x}$) to the 
set of real-valued $m$-component column vectors (here: $\vec{y}$).

\medskip
\noindent
In loose analogy to the photographic process, $\vec{x}$ can be 
viewed as representing an ``object,'' $\mathbf{A}$ a ``camera,'' 
and $\vec{y}$ the resultant ``image.''

\medskip
\noindent
Since for real-valued vectors $\vec{x}_{1},\vec{x}_{2} \in 
\mathbb{R}^{n \times 1}$ and real numbers $\lambda \in 
\mathbb{R}$, mappings defined by real-valued matrices $\mathbf{A} 
\in \mathbb{R}^{m \times n}$ exhibit the two special properties
%
\be
\lb{lin}
\fbox{$\displaystyle\begin{array}{c}
\mathbf{A}(\vec{x}_{1}+\vec{x}_{2})
= (\mathbf{A}\vec{x}_{1})+(\mathbf{A}\vec{x}_{2}) \\[5mm]
\mathbf{A}(\lambda\vec{x}_{1})
= \lambda(\mathbf{A}\vec{x}_{1}) \ ,
\end{array}
$}
\ee
%
they constitute {\bf linear mappings}.\footnote{It is important to 
note at this point that many advanced mathematical models designed 
to describe quantitative aspects of some natural and economic 
phenomena do \emph{not} satisfy the conditions (\ref{lin}), as 
they employ \emph{non-linear mappings} for this purpose. However, 
in such contexts, linear mappings often provide useful first 
approximations.}

\medskip
\noindent
We now turn to discuss the most important mathematical operations 
defined for $(m \times n)$-matrices, as well as the computational 
rules that obtain.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section[Basic concepts]%
{Basic concepts}
\lb{sec:matrech}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent
\underline{\bf Def.:}
{\bf Transpose} of a matrix

\noindent
For $\mathbf{A} \in \mathbb{R}^{m \times n}$, we define the 
process of transposing $\mathbf{A}$ by
%
\be
\fbox{$\displaystyle
\mathbf{A}^{T}\!: \quad
a_{ij}^{T} := a_{ji} \ ,
$}
\ee
%
where $i=1,\ldots,m$ und $j=1,\ldots,n$. Note that it holds that
$\mathbf{A}^{T} \in \mathbb{R}^{n \times m}$.

\medskip
\noindent
When transposing an $(m \times n)$-matrix, one simply has to 
exchange the matrix' rows with its columns (and vice versa): the 
elements of the first row become the elements of the first column, 
etc. It follows that, in particular,
%
\be
(\mathbf{A}^{T})^{T} = \mathbf{A}
\ee
%
applies.

\medskip
\noindent
Two special cases may occur for quadratic matrices (where $m=n$):
%
\begin{itemize}
	\item When $\mathbf{A}^{T}=\mathbf{A}$, one refers to 
	$\mathbf{A}$ as a {\bf symmetric matrix}.
	\item When $\mathbf{A}^{T}=-\mathbf{A}$, one refers to 
	$\mathbf{A}$ as an {\bf antisymmetric matrix}.
\end{itemize}
%

\medskip
\noindent
\underline{\bf Def.:}
{\bf Addition} of matrices

\noindent
For $\mathbf{A}, \mathbf{B} \in \mathbb{R}^{m \times n}$, the sum 
is given by
%
\be
\fbox{$
\displaystyle
\mathbf{A} + \mathbf{B} =: \mathbf{C}\!: \quad
a_{ij} + b_{ij} =: c_{ij} \ ,
$}
\ee
%
with $i=1,\ldots,m$ and $j=1,\ldots,n$.

\medskip
\noindent
Note that an addition can be performed meaningfully only for 
matrices of the \emph{same format}.

\pagebreak
\medskip
\noindent
\underline{\bf Def.:}
{\bf Rescaling} of matrices

\noindent
For $\mathbf{A} \in \mathbb{R}^{m \times n}$
and $\lambda \in \mathbb{R}\backslash \{0\}$, let
%
\be
\fbox{$\displaystyle
\lambda\mathbf{A} =: \mathbf{C}\!: \quad
\lambda a_{ij} =: c_{ij} \ ,
$}
\ee
%
where $i=1,\ldots,m$ and $j=1,\ldots,n$.

\medskip
\noindent
When rescaling a matrix, all its elements simply have to be 
multiplied by the same non-zero real number~$\lambda$.

\medskip
\noindent
{\bf Computational rules for addition and rescaling of matrices}

\noindent
For matrices $\mathbf{A}, \mathbf{B}, \mathbf{C}
\in \mathbb{R}^{m \times n}$:

\begin{enumerate}
	\item $\mathbf{A}+\mathbf{B} = \mathbf{B}+\mathbf{A}$
	\hfill ({\bf commutative addition})
	\item $\mathbf{A}+(\mathbf{B}+\mathbf{C})
	= (\mathbf{A}+\mathbf{B})+\mathbf{C}$
	\hfill ({\bf associative addition})
	\item $\mathbf{A}+\mathbf{0} = \mathbf{A}$
	\hfill ({\bf addition identity element})
	\item For every $\mathbf{A}$ and $\mathbf{B}$, there exists 
	exactly one $\mathbf{Z}$ such that 
	$\mathbf{A}+\mathbf{Z}=\mathbf{B}$.
	
	\hfill ({\bf invertibility of addition})
	\item $(\lambda\mu)\mathbf{A}=\lambda(\mu\mathbf{A})$
	with $\lambda,\mu	\in \mathbb{R} \backslash \{0\}$ 
	\hfill ({\bf associative rescaling})
	\item $1\mathbf{A}=\mathbf{A}$
	\hfill ({\bf rescaling identity element})
	\item $\lambda(\mathbf{A}+\mathbf{B})
	= \lambda\mathbf{A}+\lambda\mathbf{B}$;
	
	$(\lambda+\mu)\mathbf{A} = \lambda\mathbf{A}+\mu\mathbf{A}$
	with $\lambda, \mu	\in \mathbb{R} \backslash \{0\}$
	\hfill ({\bf distributive rescaling})
	
	\item $(\mathbf{A}+\mathbf{B})^{T} = \mathbf{A}^{T} + 
	\mathbf{B}^{T}$ \hfill ({\bf transposition rule 1})
	
	\item $(\lambda\mathbf{A})^{T} = \lambda\mathbf{A}^{T}$ with 
	$\lambda \in \mathbb{R} \backslash \{0\}$. \hfill ({\bf 
	transposition rule 2})
\end{enumerate}

\medskip
\noindent
Next we introduce a particularly useful mathematical operation for 
matrices.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section[Matrix multiplication]%
{Matrix multiplication}
\lb{sec:matmult}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent
\underline{\bf Def.:}
For a real-valued $(m \times n)$-matrix $\mathbf{A}$ and a 
real-valued $(n \times r)$-matrix $\mathbf{B}$, a {\bf matrix 
multiplication} is defined by
%
\be
\fbox{$
\displaystyle\begin{array}{c}
\mathbf{A}\mathbf{B} =: \mathbf{C} \\[5mm]
a_{i1}b_{1j}+\ldots+a_{ik}b_{kj}+\ldots
+a_{in}b_{nj}
=: \sum_{k=1}^{n}a_{ik}b_{kj} =: c_{ij} \ ,
\end{array}
$}
\ee
%
with $i=1,\ldots,m$ and $j=1,\ldots,r$, thus yielding as an 
outcome a real-valued $(m \times r)$-matrix $\mathbf{C}$.

\medskip
\noindent
The element of $\mathbf{C}$ at the intersection of the $i$th row 
and the $j$th column is determined by the computational rule
%
\be
c_{ij} = \text{Euclidian scalar product of $i$th row vector of
$\mathbf{A}$ and $j$th column vector of $\mathbf{B}$} \ .
\ee
%
It is important to realise that the definition of a matrix 
multiplication just provided depends in an essential way on the 
fact that \emph{matrix $\mathbf{A}$ on the left in the product 
needs to have as many (!) columns as matrix $\mathbf{B}$ on the 
right rows}. Otherwise, a matrix multiplication \emph{cannot} be 
defined in a meaningful way.

\medskip
\noindent
\underline{\bf GDC:} For matrices ${\tt [A]}$ and ${\tt [B]}$ 
edited beforehand, of matching formats, their matrix 
multiplication can be evaluated in mode {\tt MATRIX} $\rightarrow$ 
{\tt NAMES} by ${\tt [A]*[B]}$.

\vspace{5mm}
%\pagebreak
\noindent
{\bf Computational rules for matrix multiplication}

\noindent
For $\mathbf{A}, \mathbf{B}, \mathbf{C}$ real-valued matrices of 
correspondingly matching formats we have:

\begin{enumerate}
	\item $\mathbf{A}\mathbf{B} = \mathbf{0}$ is possible with $\mathbf{A}\neq\mathbf{0},\mathbf{B}\neq\mathbf{0}$.
	\hfill ({\bf zero divisor})
	\item $\mathbf{A}(\mathbf{B}\mathbf{C})
	= (\mathbf{A}\mathbf{B})\mathbf{C}$
	\hfill ({\bf associative matrix multiplication})
	\item $\mathbf{A}
	\underbrace{\mathbf{1}}_{\in \mathbb{R}^{n \times n}}
	=\underbrace{\mathbf{1}}_{\in \mathbb{R}^{m \times m}}
	\mathbf{A}=\mathbf{A}$
	\hfill ({\bf multiplicative identity element})
	\item $(\mathbf{A}+\mathbf{B})\mathbf{C}
	= \mathbf{A}\mathbf{C}+\mathbf{B}\mathbf{C}$
	
	$\mathbf{C}(\mathbf{A}+\mathbf{B})
	= \mathbf{C}\mathbf{A}+\mathbf{C}\mathbf{B}$
	\hfill ({\bf distributive matrix multiplication})
	\item $\mathbf{A}(\lambda\mathbf{B})
	=(\lambda\mathbf{A})\mathbf{B}
	=\lambda(\mathbf{A}\mathbf{B})$
	with $\lambda \in \mathbb{R}$ 
	\hfill ({\bf homogeneous matrix multiplication})
	\item $(\mathbf{A}\mathbf{B})^{T}
	=\mathbf{B}^{T}\mathbf{A}^{T}$
	\hfill ({\bf transposition rule}).
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
